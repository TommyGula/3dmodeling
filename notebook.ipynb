{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tomas Gula/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted depth map saved at: assets/predicted/buck.webp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Custom resize transform to maintain aspect ratio\n",
    "class ResizeAspectRatio:\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if h > w:\n",
    "            new_h = self.size\n",
    "            new_w = int(w * self.size / h)\n",
    "        else:\n",
    "            new_w = self.size\n",
    "            new_h = int(h * self.size / w)\n",
    "        return img.resize((new_w, new_h), self.interpolation)\n",
    "\n",
    "# Load the pre-trained MiDaS model\n",
    "model_type = \"DPT_Large\"  # MiDaS v3 - Large\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "# Load the model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device).eval()\n",
    "\n",
    "# Load transforms to resize and normalize the image\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "input_image_path = \"assets/buck.webp\"\n",
    "img = cv2.imread(input_image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_pil = Image.fromarray(img_rgb)  # Convert to PIL image\n",
    "input_batch = transform(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "# Predict depth\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "\n",
    "    # Resize the output to the original image size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=img.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "# Convert to numpy array\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Normalize depth map for visualization\n",
    "depth_min = depth_map.min()\n",
    "depth_max = depth_map.max()\n",
    "depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "depth_map = (depth_map * 255).astype(np.uint8)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"assets/predicted/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the depth map as an image\n",
    "output_image_path = os.path.join(output_dir, os.path.basename(input_image_path))\n",
    "cv2.imwrite(output_image_path, depth_map)\n",
    "\n",
    "# Display the depth map\n",
    "cv2.imshow(\"Depth Map\", depth_map)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Predicted depth map saved at: {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tomas Gula/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (560) must match the size of tensor b (568) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m midas(images)\n\u001b[1;32m--> 101\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\flask\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\flask\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\flask\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\flask\\lib\\site-packages\\torch\\nn\\functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\flask\\lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (560) must match the size of tensor b (568) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Custom dataset class\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, image_paths, depth_paths, transform=None, depth_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.depth_paths = depth_paths\n",
    "        self.transform = transform\n",
    "        self.depth_transform = depth_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        depth = Image.open(self.depth_paths[idx]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.depth_transform:\n",
    "            depth = self.depth_transform(depth)\n",
    "        return image, depth\n",
    "\n",
    "# Custom resize transform to maintain aspect ratio\n",
    "class ResizeAspectRatio:\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if h > w:\n",
    "            new_h = self.size\n",
    "            new_w = int(w * self.size / h)\n",
    "        else:\n",
    "            new_w = self.size\n",
    "            new_h = int(h * self.size / w)\n",
    "        return img.resize((new_w, new_h), self.interpolation)\n",
    "\n",
    "# Paths to your dataset\n",
    "image_dir = 'assets'\n",
    "depth_dir = 'assets/predicted'\n",
    "image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, img))][-1:]\n",
    "depth_paths = [os.path.join(depth_dir, img) for img in os.listdir(depth_dir) if os.path.isfile(os.path.join(depth_dir, img))][-1:]\n",
    "\n",
    "# Match depth paths to images based on filenames\n",
    "depth_paths_dict = {os.path.basename(p): p for p in depth_paths}\n",
    "depth_paths = [depth_paths_dict.get(os.path.basename(img), None) for img in image_paths]\n",
    "depth_paths = [p for p in depth_paths if p is not None]\n",
    "\n",
    "# Transform for input images and depth maps\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "depth_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = DepthDataset(image_paths, depth_paths, transform, depth_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Load the pre-trained MiDaS model\n",
    "model_type = \"DPT_Large\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "# Load the model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "#print(midas)\n",
    "\n",
    "# Fine-tuning: Make all layers trainable\n",
    "for param in midas.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(midas.parameters(), lr=1e-4)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    midas.train()\n",
    "    running_loss = 0.0\n",
    "    for images, depths in dataloader:\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = midas(images)\n",
    "        loss = criterion(outputs, depths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "print(\"Finished fine-tuning\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(midas.state_dict(), 'midas_finetuned.pth')\n",
    "\n",
    "# Load and preprocess a new image\n",
    "img = cv2.imread(\"assets/test1.jpg\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_pil = Image.fromarray(img_rgb)  # Convert to PIL image\n",
    "\n",
    "# Resize the input image to match the depth map dimensions\n",
    "original_size = img.shape[1], img.shape[0]\n",
    "resize_transform = ResizeAspectRatio(max(original_size))\n",
    "img_resized = resize_transform(img_pil)\n",
    "input_batch = transform(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "# Predict depth with the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "\n",
    "    # Resize the output to the original image size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=original_size,\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "# Convert to numpy array\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Normalize depth map for visualization\n",
    "depth_min = depth_map.min()\n",
    "depth_max = depth_map.max()\n",
    "depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "depth_map = (depth_map * 255).astype(np.uint8)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"assets/predicted/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the depth map as an image\n",
    "output_image_path = os.path.join(output_dir, os.path.basename(\"assets/test1.jpg\"))\n",
    "cv2.imwrite(output_image_path, depth_map)\n",
    "\n",
    "# Display the depth map\n",
    "cv2.imshow(\"Depth Map\", depth_map)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Predicted depth map saved at: {output_image_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
